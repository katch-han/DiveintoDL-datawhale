{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 子词嵌入（fastText）\n",
    "\n",
    "英语单词通常有其内部结构和形成方式。例如，我们可以从“dog”“dogs”和“dogcatcher”的字面上推测它们的关系。这些词都有同一个词根“dog”，但使用不同的后缀来改变词的含义。而且，这个关联可以推广至其他词汇。例如，“dog”和“dogs”的关系如同“cat”和“cats”的关系，“boy”和“boyfriend”的关系如同“girl”和“girlfriend”的关系。这一特点并非为英语所独有。在法语和西班牙语中，很多动词根据场景不同有40多种不同的形态，而在芬兰语中，一个名词可能有15种以上的形态。事实上，构词学（morphology）作为语言学的一个重要分支，研究的正是词的内部结构和形成方式。\n",
    "\n",
    "在word2vec中，我们并没有直接利用构词学中的信息。无论是在跳字模型还是连续词袋模型中，我们都将形态不同的单词用不同的向量来表示。例如，“dog”和“dogs”分别用两个不同的向量表示，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了子词嵌入（subword embedding）的方法，从而试图将构词信息引入word2vec中的跳字模型 [1]。\n",
    "\n",
    "在fastText中，每个中心词被表示成子词的集合。下面我们用单词“where”作为例子来了解子词是如何产生的。首先，我们在单词的首尾分别添加特殊字符“<”和“>”以区分作为前后缀的子词。然后，将单词当成一个由字符构成的序列来提取$n$元语法。例如，当$n=3$时，我们得到所有长度为3的子词：“<wh>”“whe”“her”“ere”“<re>”以及特殊子词“<where>”。\n",
    "\n",
    "在fastText中，对于一个词$w$，我们将它所有长度在$3 \\sim 6$的子词和特殊子词的并集记为$\\mathcal{G}_w$。那么词典则是所有词的子词集合的并集。假设词典中子词$g$的向量为$\\boldsymbol{z}_g$，那么跳字模型中词$w$的作为中心词的向量$\\boldsymbol{v}_w$则表示成\n",
    "\n",
    "$$ \\boldsymbol{v}_w = \\sum_{g\\in\\mathcal{G}_w} \\boldsymbol{z}_g. $$\n",
    "\n",
    "fastText的其余部分同跳字模型一致，不在此重复。可以看到，与跳字模型相比，fastText中词典规模更大，造成模型参数更多，同时一个词的向量需要对所有子词向量求和，继而导致计算复杂度更高。但与此同时，较生僻的复杂单词，甚至是词典中没有的单词，可能会从同它结构类似的其他词那里获取更好的词向量表示。\n",
    "\n",
    "小结\n",
    "- fastText提出了子词嵌入方法。它在word2vec中的跳字模型的基础上，将中心词向量表示成单词的子词向量之和。\n",
    "- 子词嵌入利用构词上的规律，通常可以提升生僻词表示的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全局向量的词嵌入（GloVe）\n",
    "\n",
    "word2vec中的跳字模型中使用softmax运算表达的条件概率$P(w_j\\mid w_i)$记作$q_{ij}$，即\n",
    "\n",
    "$$ q_{ij}=\\frac{\\exp(\\boldsymbol{u}_j^\\top \\boldsymbol{v}i)}{ \\sum{k \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_k^\\top \\boldsymbol{v}_i)}, $$\n",
    "\n",
    "其中$\\boldsymbol{v}_i$和$\\boldsymbol{u}_i$分别是索引为$i$的词$w_i$作为中心词和背景词时的向量表示，$\\mathcal{V} = {0, 1, \\ldots, |\\mathcal{V}|-1}$为词典索引集。\n",
    "\n",
    "对于词$w_i$，它在数据集中可能多次出现。我们将每一次以它作为中心词的所有背景词全部汇总并保留重复元素，记作多重集（multiset）$\\mathcal{C}_i$。一个元素在多重集中的个数称为该元素的重数（multiplicity）。举例来说，假设词$w_i$在数据集中出现2次：文本序列中以这2个$w_i$作为中心词的背景窗口分别包含背景词索引$2,1,5,2$和$2,3,2,1$。那么多重集$\\mathcal{C}_i = {1,1,2,2,2,2,3,5}$，其中元素1的重数为2，元素2的重数为4，元素3和5的重数均为1。将多重集$\\mathcal{C}i$中元素$j$的重数记作$x{ij}$：它表示了整个数据集中所有以$w_i$为中心词的背景窗口中词$w_j$的个数。那么，跳字模型的损失函数还可以用另一种方式表达：\n",
    "\n",
    "$$ -\\sum_{i\\in\\mathcal{V}}\\sum_{j\\in\\mathcal{V}} x_{ij} \\log,q_{ij}. $$\n",
    "\n",
    "我们将数据集中所有以词$w_i$为中心词的背景词的数量之和$\\left|\\mathcal{C}i\\right|$记为$x_i$，并将以$w_i$为中心词生成背景词$w_j$的条件概率$x{ij}/x_i$记作$p_{ij}$。我们可以进一步改写跳字模型的损失函数为\n",
    "\n",
    "$$ -\\sum_{i\\in\\mathcal{V}} x_i \\sum_{j\\in\\mathcal{V}} p_{ij} \\log,q_{ij}. $$\n",
    "\n",
    "上式中，$-\\sum_{j\\in\\mathcal{V}} p_{ij} \\log,q_{ij}$计算的是以$w_i$为中心词的背景词条件概率分布$p_{ij}$和模型预测的条件概率分布$q_{ij}$的交叉熵，且损失函数使用所有以词$w_i$为中心词的背景词的数量之和来加权。最小化上式中的损失函数会令预测的条件概率分布尽可能接近真实的条件概率分布。\n",
    "\n",
    "然而，作为常用损失函数的一种，交叉熵损失函数有时并不是好的选择。一方面，正如我们在10.2节（近似训练）中所提到的，令模型预测$q_{ij}$成为合法概率分布的代价是它在分母中基于整个词典的累加项。这很容易带来过大的计算开销。另一方面，词典中往往有大量生僻词，它们在数据集中出现的次数极少。而有关大量生僻词的条件概率分布在交叉熵损失函数中的最终预测往往并不准确。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe模型\n",
    "作为在word2vec之后提出的词嵌入模型，GloVe模型采用了平方损失，并基于该损失对跳字模型做了3点改动 ：\n",
    "\n",
    "1. 使用非概率分布的变量$p'{ij}=x{ij}$和$q'_{ij}=\\exp(\\boldsymbol{u}j^\\top \\boldsymbol{v}i)$，并对它们取对数。因此，平方损失项是$\\left(\\log,p'{ij} - \\log,q'{ij}\\right)^2 = \\left(\\boldsymbol{u}_j^\\top \\boldsymbol{v}i - \\log,x{ij}\\right)^2$。\n",
    "2. 为每个词$w_i$增加两个为标量的模型参数：中心词偏差项$b_i$和背景词偏差项$c_i$,松弛了概率定义中的规范性；\n",
    "3. 将每个损失项的权重替换成函数$h(x_{ij})$。权重函数$h(x)$是值域在$[0,1]$的单调递增函数,松弛了中心词重要性与  𝑥𝑖  线性相关的隐含假设；\n",
    "4. 用平方损失函数替代了交叉熵损失函数\n",
    "\n",
    "如此一来，GloVe模型的目标是最小化损失函数\n",
    "\n",
    "$$\\sum_{i\\in\\mathcal{V}} \\sum_{j\\in\\mathcal{V}} h(x_{ij}) \\left(\\boldsymbol{u}_j^\\top \\boldsymbol{v}i + b_i + c_j - \\log,x{ij}\\right)^2.$$\n",
    "\n",
    "其中权重函数$h(x)$的一个建议选择是：当$x < c$时（如$c = 100$），令$h(x) = (x/c)^\\alpha$（如$\\alpha = 0.75$），反之令$h(x) = 1$。因为$h(0)=0$，所以对于$x_{ij}=0$的平方损失项可以直接忽略。当使用小批量随机梯度下降来训练时，每个时间步我们随机采样小批量非零$x_{ij}$，然后计算梯度来迭代模型参数。<font color=red>这些非零$x_{ij}$是预先基于整个数据集计算得到的，包含了数据集的全局统计信息。因此，GloVe模型的命名取“全局向量”（Global Vectors）之意。</font>\n",
    "\n",
    "需要强调的是，如果词$w_i$出现在词$w_j$的背景窗口里，那么词$w_j$也会出现在词$w_i$的背景窗口里。也就是说，$x_{ij}=x_{ji}$。不同于word2vec中拟合的是非对称的条件概率$p_{ij}$，GloVe模型拟合的是对称的$\\log, x_{ij}$。因此，<font color=red>任意词的中心词向量和背景词向量在GloVe模型中是等价的。</font> 但由于初始化值的不同，同一个词最终学习到的两组词向量可能不同。当学习得到所有词向量以后，<font color=red>GloVe模型使用中心词向量与背景词向量之和作为该词的最终词向量。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 求近义词和类比词\n",
    "\n",
    "\n",
    "### 求近义词\n",
    "由于词向量空间中的余弦相似性可以衡量词语含义的相似性，可以通过寻找空间中的 k 近邻，来查询单词的近义词。\n",
    "\n",
    "### 求类比词\n",
    "除了求近义词以外，我们还可以使用预训练词向量求词与词之间的类比关系，例如“man”之于“woman”相当于“son”之于“daughter”。求类比词问题可以定义为：对于类比关系中的4个词“ 𝑎  之于  𝑏  相当于  𝑐  之于  𝑑 ”，给定前3个词  𝑎,𝑏,𝑐  求  𝑑 。求类比词的思路是，搜索与  vec(𝑐)+vec(𝑏)−vec(𝑎)  的结果向量最相似的词向量，其中  vec(𝑤)  为  𝑤  的词向量。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
