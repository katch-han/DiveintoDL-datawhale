{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot 向量表示单词，虽然它们构造起来很容易，但通常并不是一个好选择。一个主要的原因是，one-hot 词向量无法准确表达不同词之间的相似度，如我们常常使用的余弦相似度。\n",
    "\n",
    "Word2Vec 词嵌入工具的提出正是为了解决上面这个问题，它将每个词表示成一个定长的向量，并通过在语料库上的预训练使得这些向量能较好地表达不同词之间的相似和类比关系，以引入一定的语义信息。基于两种概率模型的假设，我们可以定义两种 Word2Vec 模型：\n",
    "\n",
    "Skip-Gram 跳字模型：假设背景词由中心词生成，即建模  𝑃(𝑤𝑜∣𝑤𝑐) ，其中  𝑤𝑐  为中心词， 𝑤𝑜  为任一背景词；\n",
    "\n",
    "CBOW (continuous bag-of-words) 连续词袋模型：假设中心词由背景词生成，即建模  𝑃(𝑤𝑐∣𝑤𝑜) ，其中 w𝑜  为背景词的集合。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram\n",
    "\n",
    "在跳字模型中，每个词被表示成两个dd维向量，用来计算条件概率。假设这个词在词典中索引为i，当它为中心词时向量表示为$v_i∈R^d，而为背景词时向量表示为u_i∈R^d。设中心词w_c 在词典中索引为c，背景词w_o 在词典中索引为o，$ $给定中心词生成背景词的条件概率可以通过对向量内积做softmax运算而得到：$\n",
    "$$\n",
    "P(𝑤_O|𝑤_c) = \\frac{exp(u_o v_c)}{\\sum_{i∈V} exp(u_i v_c)}\n",
    "$$\n",
    "\n",
    "$\n",
    "其中词典索引集V={0,1,…,∣V∣−1}V={0,1,…,∣V∣−1}。假设给定一个长度为T的文本序列，设时间步t的词为w(t)。$\n",
    "$假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为m时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率\n",
    "$\n",
    "\n",
    "$$\n",
    "\\prod_{t=1}^T \\prod_{-m\\leq j \\leq m,j\\neq 0}P(w^{t+j}|w^t)\n",
    "$$\n",
    "\n",
    "这里小于1和大于TT的时间步可以忽略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练跳字模型\n",
    "跳字模型的参数是每个词所对应的中心词向量和背景词向量。训练中我们通过最大化似然函数来学习模型参数，即最大似然估计。这等价于最小化以下损失函数：\n",
    "\n",
    "$$ - \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\text{log}, P(w^{(t+j)} \\mid w^{(t)}).$$\n",
    "\n",
    "如果使用随机梯度下降，那么在每一次迭代里我们随机采样一个较短的子序列来计算有关该子序列的损失，然后计算梯度来更新模型参数。梯度计算的关键是条件概率的对数有关中心词向量和背景词向量的梯度。根据定义，首先看到\n",
    "\n",
    "$$\\log P(w_o \\mid w_c) = \\boldsymbol{u}_o^\\top \\boldsymbol{v}c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)\\right)$$\n",
    "\n",
    "通过微分，我们可以得到上式中$\\boldsymbol{v}_c$的梯度\n",
    "\n",
    "$$ \\begin{aligned} \\frac{\\partial \\text{log}, P(w_o \\mid w_c)}{\\partial \\boldsymbol{v}_c} &= \\boldsymbol{u}o - \\frac{\\sum_{j \\in \\mathcal{V}} \\exp(\\boldsymbol{u}_j^\\top \\boldsymbol{v}_c)\\boldsymbol{u}j}{\\sum_{i \\in \\mathcal{V}} \\exp(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}\\ &= \\boldsymbol{u}o - \\sum_{j \\in \\mathcal{V}} \\left(\\frac{\\text{exp}(\\boldsymbol{u}_j^\\top \\boldsymbol{v}c)}{ \\sum{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}\\right) \\boldsymbol{u}_j\\ &= \\boldsymbol{u}o - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\boldsymbol{u}_j. \\end{aligned} $$\n",
    "\n",
    "它的计算需要词典中所有词以$w_c$为中心词的条件概率。有关其他词向量的梯度同理可得。\n",
    "\n",
    "训练结束后，对于词典中的任一索引为$i$的词，我们均得到该词作为中心词和背景词的两组词向量$\\boldsymbol{v}_i$和$\\boldsymbol{u}_i$。在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 连续词袋模型\n",
    "连续词袋模型与跳字模型类似。与跳字模型最大的不同在于，连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词。在同样的文本序列“the”“man”“loves”“his”“son”里，以“loves”作为中心词，且背景窗口大小为2时，连续词袋模型关心的是，给定背景词“the”“man”“his”“son”生成中心词“loves”的条件概率（如图10.2所示），也就是\n",
    "\n",
    "$$P(\\textrm{loves\"}\\mid\\textrm{the\"},\\textrm{man\"},\\textrm{his\"},\\textrm{``son\"}).$$\n",
    "\n",
    "\n",
    "因为连续词袋模型的背景词有多个，我们将这些背景词向量取平均，然后使用和跳字模型一样的方法来计算条件概率。设$\\boldsymbol{v_i}\\in\\mathbb{R}^d$和$\\boldsymbol{u_i}\\in\\mathbb{R}^d$分别表示词典中索引为$i$的词作为背景词和中心词的向量（注意符号的含义与跳字模型中的相反）。设中心词$w_c$在词典中索引为$c$，背景词$w_{o_1}, \\ldots, w_{o_{2m}}$在词典中索引为$o_1, \\ldots, o_{2m}$，那么给定背景词生成中心词的条件概率\n",
    "\n",
    "$$P(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\text{exp}\\left(\\frac{1}{2m}\\boldsymbol{u}c^\\top (\\boldsymbol{v}{o_1} + \\ldots + \\boldsymbol{v}{o{2m}}) \\right)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}\\left(\\frac{1}{2m}\\boldsymbol{u}i^\\top (\\boldsymbol{v}{o_1} + \\ldots + \\boldsymbol{v}{o{2m}}) \\right)}.$$\n",
    "\n",
    "为了让符号更加简单，我们记$\\mathcal{W}o= {w{o_1}, \\ldots, w_{o_{2m}}}$，且$\\bar{\\boldsymbol{v}}o = \\left(\\boldsymbol{v}{o_1} + \\ldots + \\boldsymbol{v}{o{2m}} \\right)/(2m)$，那么上式可以简写成\n",
    "\n",
    "$$P(w_c \\mid \\mathcal{W}_o) = \\frac{\\exp\\left(\\boldsymbol{u}_c^\\top \\bar{\\boldsymbol{v}}o\\right)}{\\sum{i \\in \\mathcal{V}} \\exp\\left(\\boldsymbol{u}_i^\\top \\bar{\\boldsymbol{v}}_o\\right)}.$$\n",
    "\n",
    "给定一个长度为$T$的文本序列，设时间步$t$的词为$w^{(t)}$，背景窗口大小为$m$。连续词袋模型的似然函数是由背景词生成任一中心词的概率\n",
    "\n",
    "$$ \\prod_{t=1}^{T} P(w^{(t)} \\mid w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).$$\n",
    "\n",
    "### 训练连续词袋模型\n",
    "训练连续词袋模型同训练跳字模型基本一致。连续词袋模型的最大似然估计等价于最小化损失函数\n",
    "\n",
    "$$ -\\sum_{t=1}^T \\text{log}, P(w^{(t)} \\mid w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).$$\n",
    "\n",
    "注意到\n",
    "\n",
    "$$\\log,P(w_c \\mid \\mathcal{W}_o) = \\boldsymbol{u}_c^\\top \\bar{\\boldsymbol{v}}o - \\log,\\left(\\sum{i \\in \\mathcal{V}} \\exp\\left(\\boldsymbol{u}_i^\\top \\bar{\\boldsymbol{v}}_o\\right)\\right).$$\n",
    "\n",
    "通过微分，我们可以计算出上式中条件概率的对数有关任一背景词向量$\\boldsymbol{v}_{o_i}$（$i = 1, \\ldots, 2m$）的梯度\n",
    "\n",
    "$$\\frac{\\partial \\log, P(w_c \\mid \\mathcal{W}o)}{\\partial \\boldsymbol{v}{o_i}} = \\frac{1}{2m} \\left(\\boldsymbol{u}c - \\sum{j \\in \\mathcal{V}} \\frac{\\exp(\\boldsymbol{u}_j^\\top \\bar{\\boldsymbol{v}}_o)\\boldsymbol{u}j}{ \\sum{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\bar{\\boldsymbol{v}}_o)} \\right) = \\frac{1}{2m}\\left(\\boldsymbol{u}c - \\sum{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\boldsymbol{u}_j \\right).$$\n",
    "\n",
    "有关其他词向量的梯度同理可得。同跳字模型不一样的一点在于，我们一般使用连续词袋模型的背景词向量作为词的表征向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结\n",
    "- 词向量是用来表示词的向量。把词映射为实数域向量的技术也叫词嵌入。\n",
    "- word2vec包含跳字模型和连续词袋模型。跳字模型假设基于中心词来生成背景词。连续词袋模型假设基于背景词来生成中心词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 近似训练\n",
    "跳字模型的核心在于使用softmax运算得到给定中心词$w_c$来生成背景词$w_o$的条件概率\n",
    "\n",
    "$$P(w_o \\mid w_c) = \\frac{\\text{exp}(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c)}{ \\sum{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)}.$$\n",
    "\n",
    "该条件概率相应的对数损失\n",
    "\n",
    "$$-\\log P(w_o \\mid w_c) = -\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c + \\log\\left(\\sum{i \\in \\mathcal{V}} \\text{exp}(\\boldsymbol{u}_i^\\top \\boldsymbol{v}_c)\\right).$$\n",
    "\n",
    "由于softmax运算考虑了背景词可能是词典$\\mathcal{V}$中的任一词，以上损失包含了词典大小数目的项的累加。不论是跳字模型还是连续词袋模型，由于条件概率使用了softmax运算，每一步的梯度计算都包含词典大小数目的项的累加。对于含几十万或上百万词的较大词典，每次的梯度计算开销可能过大。为了降低该计算复杂度，采用两种近似训练方法，即负采样（negative sampling）或层序softmax（hierarchical softmax）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 负采样\n",
    "负采样修改了原来的目标函数。给定中心词$w_c$的一个背景窗口，我们把背景词$w_o$出现在该背景窗口看作一个事件，并将该事件的概率计算为\n",
    "\n",
    "$$P(D=1\\mid w_c, w_o) = \\sigma(\\boldsymbol{u}_o^\\top \\boldsymbol{v}_c),$$\n",
    "\n",
    "其中的$\\sigma$函数与sigmoid激活函数的定义相同：\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+\\exp(-x)}.$$\n",
    "\n",
    "我们先考虑最大化文本序列中所有该事件的联合概率来训练词向量。具体来说，给定一个长度为$T$的文本序列，设时间步$t$的词为$w^{(t)}$且背景窗口大小为$m$，考虑最大化联合概率\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}).$$\n",
    "\n",
    "然而，以上模型中包含的事件仅考虑了正类样本。这导致当所有词向量相等且值为无穷大时，以上的联合概率才被最大化为1。很明显，这样的词向量毫无意义。负采样通过采样并添加负类样本使目标函数更有意义。设背景词$w_o$出现在中心词$w_c$的一个背景窗口为事件$P$，我们根据分布$P(w)$采样$K$个未出现在该背景窗口中的词，即噪声词。设噪声词$w_k$（$k=1, \\ldots, K$）不出现在中心词$w_c$的该背景窗口为事件$N_k$。假设同时含有正类样本和负类样本的事件$P, N_1, \\ldots, N_K$相互独立，负采样将以上需要最大化的仅考虑正类样本的联合概率改写为\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}),$$\n",
    "\n",
    "其中条件概率被近似表示为 $$ P(w^{(t+j)} \\mid w^{(t)}) =P(D=1\\mid w^{(t)}, w^{(t+j)})\\prod_{k=1,\\ w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k).$$\n",
    "\n",
    "设文本序列中时间步$t$的词$w^{(t)}$在词典中的索引为$i_t$，噪声词$w_k$在词典中的索引为$h_k$。有关以上条件概率的对数损失为\n",
    "\n",
    "$$ \\begin{aligned} -\\log P(w^{(t+j)} \\mid w^{(t)}) =& -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)\\ =&- \\log, \\sigma\\left(\\boldsymbol{u}{i{t+j}}^\\top \\boldsymbol{v}{i_t}\\right) - \\sum{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\boldsymbol{u}{h_k}^\\top \\boldsymbol{v}{i_t}\\right)\\right)\\ =&- \\log, \\sigma\\left(\\boldsymbol{u}{i{t+j}}^\\top \\boldsymbol{v}{i_t}\\right) - \\sum{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\boldsymbol{u}{h_k}^\\top \\boldsymbol{v}{i_t}\\right). \\end{aligned} $$\n",
    "\n",
    "现在，训练中每一步的梯度计算开销不再与词典大小相关，而与$K$线性相关。当$K$取较小的常数时，负采样在每一步的梯度计算开销较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 层序softmax\n",
    "\n",
    "层序softmax是另一种近似训练法。它使用了二叉树这一数据结构，树的每个叶结点代表词典$\\mathcal{V}$中的每个词。\n",
    "\n",
    "<img src='hierarchicalsoftmax.jpg'>\n",
    "\n",
    "假设$L(w)$为从二叉树的根结点到词$w$的叶结点的路径（包括根结点和叶结点）上的结点数。设$n(w,j)$为该路径上第$j$个结点，并设该结点的背景词向量为$\\boldsymbol{u}_{n(w,j)}$。以图10.3为例，$L(w_3) = 4$。层序softmax将跳字模型中的条件概率近似表示为\n",
    "\n",
    "$$P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [![ n(w_o, j+1) = \\text{leftChild}(n(w_o,j)) ]!] \\cdot \\boldsymbol{u}_{n(w_o,j)}^\\top \\boldsymbol{v}_c\\right),$$\n",
    "\n",
    "其中$\\sigma$函数与3.8节（多层感知机）中sigmoid激活函数的定义相同，$\\text{leftChild}(n)$是结点$n$的左子结点：如果判断$x$为真，$[![x]!] = 1$；反之$[![x]!] = -1$。 让我们计算图10.3中给定词$w_c$生成词$w_3$的条件概率。我们需要将$w_c$的词向量$\\boldsymbol{v}_c$和根结点到$w_3$路径上的非叶结点向量一一求内积。由于在二叉树中由根结点到叶结点$w_3$的路径上需要向左、向右再向左地遍历（图10.3中加粗的路径），我们得到\n",
    "\n",
    "$$P(w_3 \\mid w_c) = \\sigma(\\boldsymbol{u}_{n(w_3,1)}^\\top \\boldsymbol{v}c) \\cdot \\sigma(-\\boldsymbol{u}{n(w_3,2)}^\\top \\boldsymbol{v}c) \\cdot \\sigma(\\boldsymbol{u}{n(w_3,3)}^\\top \\boldsymbol{v}_c).$$\n",
    "\n",
    "由于$\\sigma(x)+\\sigma(-x) = 1$，给定中心词$w_c$生成词典$\\mathcal{V}$中任一词的条件概率之和为1这一条件也将满足：\n",
    "\n",
    "$$\\sum_{w \\in \\mathcal{V}} P(w \\mid w_c) = 1.$$\n",
    "\n",
    "此外，由于$L(w_o)-1$的数量级为$\\mathcal{O}(\\text{log}_2|\\mathcal{V}|)$，当词典$\\mathcal{V}$很大时，层序softmax在训练中每一步的梯度计算开销相较未使用近似训练时大幅降低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结\n",
    "- 负采样通过考虑同时含有正类样本和负类样本的相互独立事件来构造损失函数。其训练中每一步的梯度计算开销与采样的噪声词的个数线性相关。\n",
    "- 层序softmax使用了二叉树，并根据根结点到叶结点的路径来构造损失函数。其训练中每一步的梯度计算开销与词典大小的对数相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二次采样\n",
    "文本数据中一般会出现一些高频词，如英文中的“the”“a”和“in”。通常来说，在一个背景窗口中，一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。因此，训练词嵌入模型时可以对词进行二次采样。 具体来说，数据集中每个被索引词$w_i$将有一定概率被丢弃，该丢弃概率为\n",
    "\n",
    "$$ P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right), $$\n",
    "\n",
    "其中 $f(w_i)$ 是数据集中词$w_i$的个数与总词数之比，常数$t$是一个超参数（实验中设为$10^{-4}$）。可见，只有当$f(w_i) > t$时，我们才有可能在二次采样中丢弃词$w_i$，并且越高频的词被丢弃的概率越大。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
