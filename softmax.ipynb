{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax和分类模型\n",
    "\n",
    "## softmax回归的基本概念\n",
    "1. 用来处理分类问题\n",
    "2. 如果用神经网络来描绘softmax回归，同线性回归一样，是一个单层神经网络，输出层是一个全连接层。通过使用softmax运算符（softmax operator）将输出值变换成值为正且和为1的概率分布 \n",
    "$$ \\hat{y_1},\\hat{y_2},\\hat{y_3} = softmax(O_1,O_2,O_3) $$\n",
    "其中，\n",
    "$$ \n",
    "\\hat{y_1} = \\frac{exp(o_1)}{\\sum_{i=1}^3exp(o_i)} ,\n",
    "\\hat{y_2} = \\frac{exp(o_2)}{\\sum_{i=1}^3exp(o_i)} ,\n",
    "\\hat{y_3} = \\frac{exp(o_3)}{\\sum_{i=1}^3exp(o_i)}\n",
    "$$\n",
    "\n",
    "softmax回归对样本 i 分类的矢量计算表达式为\n",
    "$$ o^{(i)} = x^{(i)}W+B $$\n",
    "$$ \\hat{y}^{(i)} = softmax(O^{(i)}) $$\n",
    "\n",
    "## 交叉熵损失函数\n",
    "对于样本i，我们构造向量$ y^{(i)} $ ，使其第$ y^{(i)}  $（样本i类别的离散数值）个元素为1，其余为0。这样我们的训练目标可以设为使预测概率分布 $ \\hat{y}^{(i)} $ 尽可能接近真实的标签概率分布$ y^{(i)}  $。\n",
    "\n",
    "交叉熵（cross entropy）\n",
    "$$ H(y^{(i)},\\hat{y}^{(i)}) = - \\sum_{j=1}^{1}y^{(i)}_jlog\\hat{y}^{(i)}_j $$\n",
    "\n",
    "假设训练数据集的样本数为，交叉熵损失函数定义为\n",
    "$$ l(\\Theta)= \\frac{1}{n}\\sum_{i=1}^{n} H(y^{(i)},\\hat{y}^{(i)}) $$\n",
    "如果每个样本只有一个标签，那么交叉熵损失可以简写成 \n",
    "$ l(\\Theta)= -\\frac{1}{n}\\sum_{i=1}^{n}log\\hat{y}^{(i)}_{{y}^{(i)}} $\n",
    "\n",
    "\n",
    "# Torch 知识\n",
    "\n",
    "## torchvision\n",
    "    是服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。torchvision主要由以下几部分构成：\n",
    "\n",
    ". torchvision.datasets: 一些加载数据的函数及常用的数据集接口；\n",
    ". torchvision.models: 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；\n",
    ". torchvision.transforms: 常用的图片变换，例如裁剪、旋转等；\n",
    ". torchvision.utils: 其他的一些有用的方法。\n",
    "\n",
    "## 对多维Tensor按维度操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T12:30:41.456118Z",
     "start_time": "2020-02-13T12:30:41.374682Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 7, 9]])\n",
      "tensor([[ 6],\n",
      "        [15]])\n",
      "tensor([5, 7, 9])\n",
      "tensor([ 6, 15])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(X.sum(dim=0, keepdim=True))  # dim为0，按照相同的列求和，并在结果中保留列特征\n",
    "print(X.sum(dim=1, keepdim=True))  # dim为1，按照相同的行求和，并在结果中保留行特征\n",
    "print(X.sum(dim=0, keepdim=False)) # dim为0，按照相同的列求和，不在结果中保留列特征\n",
    "print(X.sum(dim=1, keepdim=False)) # dim为1，按照相同的行求和，不在结果中保留行特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T13:23:23.726482Z",
     "start_time": "2020-02-13T13:23:23.538460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/katch/Documents/learn/DataWhale/动手深度学习/task1\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T13:24:52.348675Z",
     "start_time": "2020-02-13T13:24:52.344126Z"
    }
   },
   "outputs": [],
   "source": [
    "#softmax的简洁实现\n",
    "# 加载各种包或者模块\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"/Users/katch/Documents/learn/DataWhale/动手深度学习/input\")\n",
    "# import d2lzh1981 as d2l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T15:09:17.921268Z",
     "start_time": "2020-02-13T15:08:54.444865Z"
    }
   },
   "outputs": [],
   "source": [
    "#获取数据\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='/Users/katch/Documents/learn/DataWhale/动手深度学习/input/FashionMNIST2065', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='/Users/katch/Documents/learn/DataWhale/动手深度学习/input/FashionMNIST2065', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 256\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义网络模型\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "    def forward(self, x): # x 的形状: (batch, 1, 28, 28)\n",
    "        y = self.linear(x.view(x.shape[0], -1))\n",
    "        return y\n",
    "    \n",
    "# net = LinearNet(num_inputs, num_outputs)\n",
    "\n",
    "class FlattenLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenLayer, self).__init__()\n",
    "    def forward(self, x): # x 的形状: (batch, *, *, ...)\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "from collections import OrderedDict\n",
    "net = nn.Sequential(\n",
    "        # FlattenLayer(),\n",
    "        # LinearNet(num_inputs, num_outputs) \n",
    "        OrderedDict([\n",
    "           ('flatten', FlattenLayer()),\n",
    "           ('linear', nn.Linear(num_inputs, num_outputs))]) # 或者写成我们自己定义的 LinearNet(num_inputs, num_outputs) 也可以\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化模型参数\n",
    "init.normal_(net.linear.weight, mean=0, std=0.01)\n",
    "init.constant_(net.linear.bias, val=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义损失函数\n",
    "loss = nn.CrossEntropyLoss() # 下面是他的函数原型\n",
    "# class torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义优化函数\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1) # 下面是函数原型\n",
    "# class torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "            \n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "            \n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.step() \n",
    "            \n",
    "            \n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "        \n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
