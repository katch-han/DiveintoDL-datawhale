{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练误差和泛化误差\n",
    "    训练误差（training error）指模型在训练数据集上表现出的误差，泛化误差（generalization error）指模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。使用损失函数来计算训练误差和泛化误差，例如线性回归用到的平方损失函数和softmax回归用到的交叉熵损失函数\n",
    "    \n",
    "    在机器学习里，我们通常假设训练数据集（训练题）和测试数据集（测试题）里的每一个样本都是从同一个概率分布中相互独立地生成的。基于该独立同分布假设，给定任意一个机器学习模型（含参数），它的训练误差的期望和泛化误差都是一样的。但是模型的参数是通过在训练数据集上训练模型而学习出的，参数的选择依据了最小化训练误差。所以，训练误差的期望小于或等于泛化误差。也就是说，一般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。由于无法从训练误差估计泛化误差，一味地降低训练误差并不意味着泛化误差一定会降低。\n",
    "\n",
    "    机器学习模型应关注降低泛化误差。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 验证数据集\n",
    "    从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。\n",
    "    \n",
    "### K折交叉验证\n",
    "    由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（KK-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K−1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 欠拟合和过拟合\n",
    "    模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；\n",
    "    模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。\n",
    "    在实践中，我们要尽可能同时应对欠拟合和过拟合。\n",
    "    给定训练数据集，如果模型的复杂度过低，很容易出现欠拟合；如果模型复杂度过高，很容易出现过拟合。应对欠拟合和过拟合的一个办法是针对数据集选择合适复杂度的模型。\n",
    "    影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 权重衰减\n",
    "    权重衰减等价于  𝐿2  范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。\n",
    "    \n",
    "### L2 范数正则化（regularization）\n",
    "    𝐿2 范数正则化在模型原损失函数基础上添加 𝐿2 范数惩罚项，从而得到训练所需要最小化的函数。 𝐿2 范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。\n",
    "$$ 𝐿(w_1,w_2,b) + \\frac{\\lambda}{2n}||w||^2 $$\n",
    "    其中超参数 𝜆>0 。当权重参数均为0时，惩罚项最小。当 𝜆 较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当 𝜆 设为0时，惩罚项完全不起作用。通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。\n",
    "\n",
    "### dropout\n",
    "    深度学习模型常常使用丢弃法（dropout）来应对过拟合问题；当对神经网络的某一隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为 𝑝 ，那么有 𝑝 的概率 ℎ𝑖 会被清零，有 1−𝑝 的概率 ℎ𝑖 会除以 1−𝑝 做拉伸。丢弃概率是丢弃法的超参数。也可以理解为减小了模型的复杂度；在测试模型时，一般不使用丢弃法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
