{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一维梯度下降\n",
    "\n",
    "假设连续可导的函数f:R→Rf:R→R的输入和输出都是标量。给定绝对值足够小的数ϵϵ，根据泰勒展开公式，我们得到以下的近似：\n",
    "\n",
    "f(x+ϵ)≈f(x)+ϵf′(x)\n",
    "\n",
    "这里f′(x)是函数f在x处的梯度。一维函数的梯度是一个标量，也称导数。\n",
    "\n",
    "接下来，找到一个常数η>0，使得∣ηf′(x)∣足够小，那么可以将ϵϵ替换为−ηf′(x)并得到\n",
    "$$ f(x−ηf′(x))≈f(x)−ηf′(x)^2 $$\n",
    "如果导数f′(x)≠0，那么ηf′(x)2>0,所以f(x−ηf′(x))≲f(x).\n",
    "这意味着，如果通过  x←x−ηf′(x)  来迭代x，函数  f(x)  的值可能会降低。因此在梯度下降中，我们先选取一个初始值x和常数η>0，然后不断通过上式来迭代x，直到达到停止条件，例如 $ f′(x)^2 $ 的值已足够小或迭代次数已达到某个值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率\n",
    "\n",
    "梯度下降算法中的正数η通常叫作学习率。这是一个超参数，需要人工设定。如果使用过小的学习率，会导致x更新缓慢从而需要更多的迭代才能得到较好的解。\n",
    "如果使用过大的学习率，∣ηf′(x)∣ 可能会过大从而使前面提到的一阶泰勒展开公式不再成立：这时我们无法保证迭代x会降低f(x)的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多维梯度下降\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})=\\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_{1}}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_{2}}, \\dots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_{d}}\\right]^{\\top}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}+\\epsilon)=f(\\mathbf{x})+\\epsilon^{\\top} \\nabla f(\\mathbf{x})+\\mathcal{O}\\left(\\|\\epsilon\\|^{2}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f(\\mathbf{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自适应方法\n",
    "\n",
    "### 牛顿法\n",
    "在 $x + \\epsilon$ 处泰勒展开：\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}+\\epsilon)=f(\\mathbf{x})+\\epsilon^{\\top} \\nabla f(\\mathbf{x})+\\frac{1}{2} \\epsilon^{\\top} \\nabla \\nabla^{\\top} f(\\mathbf{x}) \\epsilon+\\mathcal{O}\\left(\\|\\epsilon\\|^{3}\\right)\n",
    "$$\n",
    "\n",
    "最小值点处满足: $\\nabla f(\\mathbf{x})=0$, 即我们希望 $\\nabla f(\\mathbf{x} + \\epsilon)=0$, 对上式关于 $\\epsilon$ 求导，忽略高阶无穷小，有：\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})+\\boldsymbol{H}_{f} \\boldsymbol{\\epsilon}=0 \\text { and hence } \\epsilon=-\\boldsymbol{H}_{f}^{-1} \\nabla f(\\mathbf{x})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降\n",
    "在深度学习里，目标函数通常是训练数据集中有关各个样本的损失函数的平均。设$f_i(x)$是有关索引为i的训练数据样本的损失函数，n是训练数据样本数，x是模型的参数向量，那么目标函数定义为\n",
    "$$\n",
    "f(\\mathbf{x})=\\frac{1}{n} \\sum_{i=1}^{n} f_{i}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "目标函数在xx处的梯度计算为\n",
    "\n",
    "$$\n",
    "\\nabla f(\\mathbf{x})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_{i}(\\mathbf{x})\n",
    "$$\n",
    "如果使用梯度下降，每次自变量迭代的计算开销为O(n)，它随着n线性增长。因此，当训练数据样本数很大时，梯度下降每次迭代的计算开销很高\n",
    "\n",
    "随机梯度下降（stochastic gradient descent，SGD）减少了每次迭代的计算开销。在随机梯度下降的每次迭代中，我们随机均匀采样的一个样本索引i∈{1,…,n}i∈{1,…,n}，并计算梯度∇fi(x)来迭代x：\n",
    "\n",
    "$$\n",
    "\\mathbf{x} \\leftarrow \\mathbf{x}-\\eta \\nabla f_{i}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "这里η同样是学习率。可以看到每次迭代的计算开销从梯度下降的O(n)降到了常数O(1)。值得强调的是，随机梯度∇fi(x)是对梯度∇f(x)的无偏估计：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{i} \\nabla f_{i}(\\mathbf{x})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_{i}(\\mathbf{x})=\\nabla f(\\mathbf{x})\n",
    "$$\n",
    "这意味着，平均来说，随机梯度是对梯度的一个良好的估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小批量随机梯度下降\n",
    "在每一次迭代中，梯度下降使用整个训练数据集来计算梯度，因此它有时也被称为批量梯度下降（batch gradient descent）。而随机梯度下降在每次迭代中只随机采样一个样本来计算梯度。我们还可以在每轮迭代中随机均匀采样多个样本来组成一个小批量，然后使用这个小批量来计算梯度。\n",
    "\n",
    "设目标函数$ f(x):R^d→R $。在迭代开始前的时间步设为0。该时间步的自变量记为$ x_0∈R^d $，通常由随机初始化得到。在接下来的每一个时间步t>0中，小批量随机梯度下降随机均匀采样一个由训练数据样本索引组成的小批量$B_t$。我们可以通过重复采样（sampling with replacement）或者不重复采样（sampling without replacement）得到一个小批量中的各个样本。前者允许同一个小批量中出现重复的样本，后者则不允许如此，且更常见。对于这两者间的任一种方式，都可以使用\n",
    "$$\n",
    "g_t \\leftarrow \\delta \\nabla f_{B_t}(x_{t-1}) = \\frac{1}{|B|}\\nabla f_{i}(x_{t-1})\n",
    "$$\n",
    "来计算时间步tt的小批量 $B_t $上目标函数位于$x_{t-1}$处的梯度$g_t$。这里∣B∣代表批量大小，即小批量中样本的个数，是一个超参数。同随机梯度一样，重复采样所得的小批量随机梯度$g_t$ 也是对梯度$∇f(x_{t−1})$的无偏估计。给定学习率ηt（取正数），小批量随机梯度下降对自变量的迭代如下：\n",
    "$$\n",
    "x_t \\rightarrow x_{t-1} - η_tg_t\n",
    "$$\n",
    " \n",
    " \n",
    "- 小批量随机梯度每次随机均匀采样一个小批量的训练样本来计算梯度。\n",
    "- 在实际中，（小批量）随机梯度下降的学习率可以在迭代过程中自我衰减。\n",
    "- 通常，小批量随机梯度在每个迭代周期的耗时介于梯度下降和随机梯度下降的耗时之间。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
