{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量归一化\n",
    "批量归一化（batch normalization）层能让较深的神经网络的训练变得更加容易 。对输入数据做标准化处理，处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。\n",
    "\n",
    "通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。\n",
    "\n",
    "批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量归一化层\n",
    "\n",
    "#### 对全连接层做批量归一化\n",
    "通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为uu，权重参数和偏差参数分别为W和b，，激活函数为ϕ。设批量归一化的运算符为BN。那么，使用批量归一化的全连接层的输出为：ϕ(BN(x))\n",
    "其中批量归一化输入由x仿射变换 x=Wu+bx 得到。考虑一个由m个样本组成的小批量，仿射变换的输出为一个新的小批量B={x(1),…,x(m）}。它们正是批量归一化层的输入。对于小批量 B 中任意样本 x(i)∈Rd,1≤i≤m，批量归一化层的输出同样是d维向量 y(i)=BN(x(i)), 并由以下几步求得。首先，对小批量BB求均值和方差：\n",
    "$$\n",
    "\\boldsymbol{\\mu}_\\mathcal{B} \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m} \\boldsymbol{x}^{(i)},\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\sigma}_\\mathcal{B}^2 \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}(\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B})^2,\n",
    "$$\n",
    "其中的平方计算是按元素求平方。接下来，使用按元素开方和按元素除法对$x^{(i)}$标准化：\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{x}}^{(i)} \\leftarrow \\frac{\\boldsymbol{x}^{(i)} - \\boldsymbol{\\mu}_\\mathcal{B}}{\\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}},\n",
    "$$\n",
    "\n",
    "这里ϵ>0是一个很小的常数，保证分母大于0。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数γ和偏移（shift）参数β。这两个参数和 $x^{(i)}$ 形状相同，皆为d维向量。它们与 $x^{(i)}$ 分别做按元素乘法（符号⊙）和加法计算：\n",
    "\n",
    "\n",
    "$$\n",
    "{\\boldsymbol{y}}^{(i)} \\leftarrow \\boldsymbol{\\gamma} \\odot\n",
    "\\hat{\\boldsymbol{x}}^{(i)} + \\boldsymbol{\\beta}.\n",
    "$$\n",
    "\n",
    "至此，我们得到了 $x^{(i)}$ 的批量归一化的输出y(i)。 值得注意的是，可学习的拉伸和偏移参数保留了不对$x^{(i)}$ 做批量归一化的可能：此时只需学出$\\boldsymbol{\\gamma} = \\sqrt{\\boldsymbol{\\sigma}_\\mathcal{B}^2 + \\epsilon}$和$\\boldsymbol{\\beta} = \\boldsymbol{\\mu}_\\mathcal{B}$ 。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对卷积层做批量归一化\n",
    "\n",
    "位置：卷积计算之后、应⽤激活函数之前。  \n",
    "如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数。\n",
    "计算：对单通道，batchsize=m,卷积计算输出=pxq\n",
    "对该通道中m×p×q个元素同时做批量归一化,使用相同的均值和方差。\n",
    "\n",
    "对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。设小批量中有 m个样本。在单个通道上，假设卷积计算输出的高和宽分别为 p和 q 。我们需要对该通道中 m×p×q个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中 m×p×q个元素的均值和方差\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测时的批量归⼀化\n",
    "\n",
    "训练：以batch为单位,对每个batch计算均值和方差。  \n",
    "预测：用移动平均估算整个训练数据集的样本均值和方差。\n",
    "\n",
    "使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 残差网络（ResNet）\n",
    "对神经网络模型添加新的层，充分训练后的模型是否只可能更有效地降低训练误差？理论上，原模型解的空间只是新模型解的空间的子空间。也就是说，如果我们能将新添加的层训练成恒等映射f(x)=x，新模型和原模型将同样有效。由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。\n",
    "\n",
    "### 残差块\n",
    "在神经网络局部，设输入为x。假设我们希望学出的理想映射为f(x)，从而作为上方激活函数的输入。左图虚线框中的部分需要直接拟合出该映射f(x)，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射f(x)−x。残差映射在实际中往往更容易优化。以恒等映射f(x)=x作为我们希望学出的理想映射f(x)。我们只需将图中右图虚线框内上方的加权运算（如仿射）的权重和偏差参数学成0，那么f(x)即为恒等映射。实际中，当理想映射f(x)极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。右图也是ResNet的基础块，即残差块（residual block）。<font color=red>在残差块中，输入可通过跨层的数据线路更快地向前传播。</font>\n",
    "\n",
    "<img src='resNet.jpg'>\n",
    "\n",
    "ResNet沿用了VGG全3×3卷积层的设计。残差块里首先有2个有相同输出通道数的3×3卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的1×1卷积层来将输入变换成需要的形状后再做相加运算。\n",
    "\n",
    "### ResNet\n",
    "\n",
    "卷积(64,7x7,3)  \n",
    "批量一体化  \n",
    "最大池化(3x3,2)  \n",
    "残差块x4 (通过步幅为2的残差块在每个模块之间减小高和宽)\n",
    "全局平均池化\n",
    "全连接\n",
    "\n",
    "ResNet的前两层跟之前介绍的GoogLeNet中的一样：在输出通道数为64、步幅为2的7×7卷积层后接步幅为2的3×3的最大池化层。不同之处在于ResNet每个卷积层后增加的批量归一化层。\n",
    "\n",
    "net = tf.keras.models.Sequential(\n",
    "    [layers.Conv2D(64, kernel_size=7, strides=2, padding='same'),\n",
    "    layers.BatchNormalization(), layers.Activation('relu'),\n",
    "    layers.MaxPool2D(pool_size=3, strides=2, padding='same')])\n",
    "\n",
    "一个模块的通道数同输入通道数一致。由于之前已经使用了步幅为2的最大池化层，所以无须减小高和宽。之后的每个模块在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半。\n",
    "\n",
    "通过配置不同的通道数和模块里的残差块数可以得到不同的ResNet模型，例如更深的含152层的ResNet-152;或者每个模块里有4个卷积层（不计算 1×1卷积层），加上最开始的卷积层和最后的全连接层，共计18层。这个模型通常也被称为ResNet-18。 虽然ResNet的主体架构跟GoogLeNet的类似，但ResNet结构更简单，修改也更方便。这些因素都导致了ResNet迅速被广泛使用\n",
    "\n",
    "### 小结\n",
    "- 残差块通过跨层的数据通道从而能够训练出有效的深度神经网络。\n",
    "- ResNet深刻影响了后来的深度神经网络的设计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 稠密连接网络（DenseNet）\n",
    "ResNet中的跨层连接设计引申出了数个后续工作。其中的一个是稠密连接网络（DenseNet）\n",
    "\n",
    "<img src='DenseNet.jpg' width=60%>\n",
    "\n",
    "上图中将部分前后相邻的运算抽象为模块 A 和模块B。与ResNet的主要区别在于，DenseNet里模块B的输出不是像ResNet那样和模块A的输出相加，而是在通道维上连结。这样模块A的输出可以直接传入模块B后面的层。在这个设计里，模块A直接跟模块B后面的所有层连接在了一起。这也是它被称为“稠密连接”的原因。\n",
    "DenseNet的主要构建模块是稠密块（dense block）和过渡层（transition layer）。前者定义了输入和输出是如何连结的，后者则用来控制通道数，使之不过大。\n",
    "\n",
    "###  稠密块\n",
    "DenseNet使用了ResNet改良版的“批量归一化、激活和卷积”结构；在前向计算时，将每块的输入和输出在通道维上连结。\n",
    "\n",
    "### 过渡层\n",
    "由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡层用来控制模型复杂度。它通过1×1卷积层来减小通道数，并使用步幅为2的平均池化层减半高和宽，从而进一步降低模型复杂度。\n",
    "\n",
    "### DenseNet模型\n",
    "DenseNet首先使用同ResNet一样的单卷积层和最大池化层。类似于ResNet接下来使用的4个残差块，DenseNet使用的是4个稠密块。同ResNet一样，我们可以设置每个稠密块使用多少个卷积层。这里我们设成4，从而与ResNet-18保持一致。稠密块里的卷积层通道数（即增长率）设为32，所以每个稠密块将增加128个通道。\n",
    "\n",
    "ResNet里通过步幅为2的残差块在每个模块之间减小高和宽。DenseNet则使用过渡层来减半高和宽，并减半通道数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
